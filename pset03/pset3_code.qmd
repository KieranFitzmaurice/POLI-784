---
title: "POLI 784 - Problem Set #2"
author: "Nico Cardenas-Miller, Kieran Fitzmason, Otilda Harris"
date: today
format: html
editor: visual
---

# Question 1

```{r}
#| echo: false
#| include: false

# Install required packages if you haven't already
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("dagitty")) install.packages("dagitty")
if (!require("ggdag")) install.packages("ggdag")

# Load packages into environment
library(tidyverse)
library(dagitty)
library(ggdag)
```

## Part A

Here, we simulate a dataset that includes variables representing our treatment $D$, outcome of interest $Y$, observed covariates $\mathbf{X}$, and an unobserved covariate $U_1$. The causal relationships between these variables is represented below as a directed acyclic graph (DAG).

### DAG representation of causal relationships

```{r}

# Random seed has an effect on the placement of variables in the DAG plot. 
# Pick one that makes the DAG look pretty. 
set.seed(1200)

# Represent relationship between variables as a DAG
dag1A <- dagify(
  X1 ~ U1,
  D ~ X1 + X2, 
  X3 ~ D,
  Y ~ D + X3 + X2 + U1,
  X4 ~ D + Y, 
  exposure = "D",
  outcome = "Y",
  latent = c("U1")
)

# Styled plot
ggdag_status(dag1A) +
  theme_dag() +
  labs(title = "DAG 1A")
```

### Simulated data

```{r}

# Set random seed for reproducibility
set.seed(42)

## Simulate data

# Specify sample size
N <- 1000

# Specify direct effect of D on Y
tau_DY <- 1.0

# Specify indirect effect of D on Y via X3
tau_DX3 <- 0.5
tau_X3Y <- 1.0

# Calculate direct and total effect of D on Y
tau_direct <- tau_DY
tau_indirect <- tau_DX3*tau_X3Y
tau_total <- tau_direct + tau_indirect

# Initialize dataframe
df <- tibble(U1 = rep(NA,N),
             X1 = NA,
             X2 = NA,
             X3 = NA,
             X4 = NA,
             D = NA,
             Y = NA)

# Populate dataframe
df$U1 <- rbinom(N,1,0.5)
df$X1 <- 1 + 0.5*df$U1 + rnorm(N,0,1)
df$X2 <- rbinom(N,1,0.5)
df$D <- 1.25 + 0.5*df$X1 + 0.25*df$X2 + rnorm(N,0,0.1)
df$X3 <- 1 + tau_DX3*df$D + rnorm(N,0,0.1)
df$Y <- 5 + tau_DY*df$D + tau_X3Y*df$X3 + 0.1*df$X2 + 0.1*df$U1 + rnorm(N,0,0.25)
df$X4 <- -0.5 + 2*df$D + 0.75*df$Y + rnorm(N,0,1)

# Display to console 
head(df)
```

## Part B

Here, we estimate the direct effect of $D$ on $Y$ using linear regression. In order to block the back-door paths between $D$ and $Y$, we must condition on $X_1$ and $X_2$ by including them as variables in our regression model. Because we are interested in isolating the direct effect of $D$ on $Y$ we will also condition on $X_3$, which acts as a mediator. This is analogous to asking: how would changing the value of $D$ by one unit affect $Y$ if we were to hold $X_3$ constant?

We can also use the `dagitty::adjustmentSets` function to verify that this is the correct set of variables to condition on given the structure of our causal graph.

```{r}

# Get the set of variables to condition on when estimating the direct effect of D on Y. 
C_direct <- adjustmentSets(dag1A,effect="direct")
print(paste("Conditioning set:",lapply(C_direct, paste, collapse = ",")))

# Estimate direct effect of D on Y 
model <- lm(Y ~ D + X1 + X2 + X3, data = df)
tau_direct_hat <- as.numeric(model$coefficients["D"])

# Print results to console
print(paste("Estimated direct effect of D on Y:",tau_direct_hat))
print(paste("Actual direct effect of D on Y:",tau_direct))
```

As shown above, the linear model is able to recover an unbiased estimate of the direct effect of $D$ on $Y$ when we condition on $X_1$, $X_2$, and $X_3$. Our estimated coefficient is not the exact same as the true effect due to the finite sample size, but repeated simulation with different random seeds would show that our estimation approach is unbiased in expectation.

## Part C

Here, we estimate the total effect of $D$ on $Y$. We will condition on $X_1$ and $X_2$ to block the back-door paths between $D$ and $Y$ as we did in part B; however, we will not include $X_3$ in our conditioning set because we are interested in capturing how $D$ impacts $Y$ both directly and through its interactions with mediator variables.

```{r}

# Get the set of variables to condition on when estimating the total effect of D on Y. 
C_total <- adjustmentSets(dag1A,effect="total")
print(paste("Conditioning set:",lapply(C_total, paste, collapse = ",")))

# Estimate total effect of D on Y 
model <- lm(Y ~ D + X1 + X2, data = df)
tau_total_hat <- as.numeric(model$coefficients["D"])

# Print results to console
print(paste("Estimated total effect of D on Y:",tau_total_hat))
print(paste("Actual total effect of D on Y:",tau_total))
```

As shown above, the linear model is able to recover an unbiased estimate of the total effect of $D$ on $Y$ when we condition on $X_1$ and $X_2$. Our estimated coefficient is not the exact same as the true effect due to the finite sample size, but repeated simulation with different random seeds would show that our estimation approach is unbiased in expectation.

## Part D

Here, we evaluate the impact of adding a $X_4$ to our conditioning set on estimates of the total effect of $D$ on $Y$.

```{r}

# Print conditioning set
print("Conditioning set: X1,X2,X4")

# Estimate total effect of D on Y 
model <- lm(Y ~ D + X1 + X2 + X4, data = df)
tau_total_hat <- as.numeric(model$coefficients["D"])

# Print results to console
print(paste("Estimated total effect of D on Y:",tau_total_hat))
print(paste("Actual total effect of D on Y:",tau_total))
```

By including $X_4$ in our conditioning set, we have opened up a back-door path between $D$ and $Y$ and introduced bias into our causal effect estimate, which now underestimates the true effect. Previously, this back-door path was blocked because $X_4$ is a collider variable. Overcontrolling for a collider variable in this manner is commonly referred to as "collider bias."

## Part E

Here, we modify the DAG from Part A by (1) removing the direct effect of $D$ on $Y$, (2) removing $X_1$ from the graph, and (3) adding a direct effect of $U_1$ on $D$.

```{r}

# Represent relationship between variables as a DAG
dag1E <- dagify(
  D ~ U1 + X2, 
  X3 ~ D,
  Y ~ X3 + X2 + U1,
  X4 ~ D + Y, 
  exposure = "D",
  outcome = "Y",
  latent = c("U1")
)

# Styled plot
ggdag_status(dag1E) +
  theme_dag() +
  labs(title = "DAG 1E")
```

In the above DAG, the back-door criterion would tell us that in order to estimate the causal effect of $D$ on $Y$, we must condition on $X_2$ and $U_1$. Both of these variables are confounders (i.e., they influence both the treatment and the outcome); however, only $X_2$ is observed, which means that the back-door criterion fails.

In order to estimate the causal effect of $D$ on $Y$, we need to take a different approach known as the front-door criterion. This involves two steps: (1) estimating the effect of $D$ on $X_3$, without conditioning on any other variables and (2) estimating the effect of $X_3$ on $Y$ while conditioning on $D$. Both of these effects will be unbiased because all back-door paths between $D$ and $X_3$ are blocked by colliders, and $D$ blocks all back-door paths between $X_3$ and $Y$. As such, multiplying these two effects together should give us an unbiased estimate of the effect of $D$ on $Y$.

## Part F

Here, we modify the DAG from Part E by adding an additional latent variable $U_2$ that directly influences both $X_2$ and $Y$.

```{r}

# Represent relationship between variables as a DAG
dag1F <- dagify(
  D ~ U1 + X2,
  X2 ~ U2,
  X3 ~ D,
  Y ~ X3 + X2 + U1 + U2,
  X4 ~ D + Y, 
  exposure = "D",
  outcome = "Y",
  latent = c("U1","U2")
)

# Styled plot
ggdag_status(dag1F) +
  theme_dag() +
  labs(title = "DAG 1F")
```

Adding $U_2$ doesn't actually change much: the back-door criterion would still tell us to condition on $X_2$ and $U_1$, which blocks the back-door path passing through $U_2$. As in Part E, the back-door criterion fails since $U_1$ is unobserved, but we can still use the front-door criterion to estimate the causal effect of $D$ on $Y$.

If the new DAG represents the true data-generating process, then this implies that the DAG from Part E is technically not a valid causal DAG—at least not according to the definition given in Chapter 6 of [Hernán and Robins (2020)](https://static1.squarespace.com/static/675db8b0dd37046447128f5f/t/691fb7706ce66332f0b44467/1763686256720/hernanrobins_WhatIf_21nov25.pdf#page=85.00). Their definition states that one of the requirements of a causal graph is that "all common causes, even if unmeasured, of any pair of variables on the graph are themselves on the graph." Because the DAG in Part E omits $U_2$, which is a common cause of $X_2$ and $Y$, it does not meet their definition of a causal graph. However, this is mainly a technical point; even though the DAG in Part E omits $U_2$, it still includes enough relevant variables for us to design a strategy for estimating the effect of $D$ on $Y$.

## Part G

Based on the results of Parts A-F, we can draw the following conclusions:

-   In general, we should always control for confounders since this closes back-door paths.

-   In general, we should never control for colliders as this opens up back-door paths.

-   We should only control for mediators when attempting to isolate direct effects from total effects.

These guidelines assume that our causal DAG is correctly specified and captures all variables that are relevant to the data-generating process (including those that are unobserved). In practice, all models are wrong, and there's always a chance that an important variable or causal relationship was omitted or misspecified. While we can reduce these risks by carefully reviewing the literature and eliciting expert opinion when constructing our DAG, it's impossible to eliminate them completely. As such, it is a good idea to construct multiple DAGs that reflect uncertainties in the structure of the data-generating process. In some cases, it may be possible to design estimation strategies that perform well across multiple representations of the data-generating process.

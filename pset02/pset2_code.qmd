---
title: "POLI 784 - Problem Set #1"
author: "Yiheng Chen, Kieran Fitzmason, Kat Gruschow"
date: today
format: html
---

# Question 1

In this question, we examine the conditions under which the Horvitz-Thompson (HT) and Hajek (HA) estimators are equivalent to the difference-in-means (DIM) estimator. We represent the DIM estimator as follows:

$$ \hat{\tau}_{DIM} = \bar{Y_t} - \bar{Y_c} = \frac{\sum_{i=1}^{N} D_i Y_i}{\sum_{i=1}^{N} D_i} - \frac{\sum_{i=1}^{N} (1-D_i) Y_i}{\sum_{i=1}^{N} (1-D_i)} $$ {#eq-dim}

where $N$ is the sample size, $D_i$ is a binary variable denoting whether a unit was assigned to treatment ($D_i=1$) or control ($D_i=0$) and $Y_i$ is the observed outcome of each unit.

## Part A

The canonical form of the HT estimator is as follows:

$$
\hat{\tau}_{HT} = \frac{1}{N}\sum_{i=1}^{N}\left(\frac{D_i Y_i}{p_i} - \frac{(1-D_i)Y_i}{(1-p_i)}\right)
$$ {#eq-ht-canonical}

where $p_i$ is the probability of an individual unit being assigned to treatment. When a fixed number $N_t$ of $N$ units are assigned to treatment, we can re-write the probability of being assigned to treatment or control as follows:

$$
p_i = \frac{N_t}{N}
$$ {#eq-ht-treatment-prob}

$$
1-p_i = 1 - \frac{N_t}{N} = \frac{N - N_t}{N} = \frac{N_c}{N}
$$ {#eq-ht-control-prob}

where $N_t$ and $N_c$ are the number of treated and control units respectively. By substituting @eq-ht-treatment-prob and @eq-ht-control-prob into @eq-ht-canonical, we can rewrite the HT estimator as follows:

$$
\hat{\tau}_{HT} = \frac{1}{N}\sum_{i=1}^{N}\left(\frac{D_i Y_i}{N_t/N} - \frac{(1-D_i)Y_i}{N_c/N}\right) = \frac{1}{N_t}\sum_{i=1}^{N}D_iY_i - \frac{1}{N_c}\sum_{i=1}^{N}(1-D_i)Y_i
$$ {#eq-ht-substitution}

By recognizing that we can rewrite $N_t$ and $N_c$ as the summation of $D_i$ and $(1-D_i)$ respectively, we can reduce @eq-ht-substitution into a form that is equivalent to the DIM estimator:

$$
\hat{\tau}_{HT} = \frac{\sum_{i=1}^{N} D_i Y_i}{\sum_{i=1}^{N} D_i} - \frac{\sum_{i=1}^{N} (1-D_i) Y_i}{\sum_{i=1}^{N} (1-D_i)}
$$ {#eq-ht-equivalent}

This equivalence should hold for complete random assignment since this treatment assignment mechanism ensures that $N_t$ and $p_i$ are both fixed constants. It will not hold for simple random assignment because $N_t$ is a binomially distributed random variable and not a fixed constant under this assignment mechanism.

## Part B

The canonical form of the HA estimator is as follows:

$$
\hat{\tau}_{HA} = \frac{\left(\sum_{i=1}^{N}\frac{D_iY_i}{p_i}\right)}{\left(\sum_{i=1}^{N}\frac{D_i}{p_i}\right)} - \frac{\left(\sum_{i=1}^{N}\frac{(1-D_i)Y_i}{(1-p_i)}\right)}{\left(\sum_{i=1}^{N}\frac{(1-D_i)}{(1-p_i)}\right)}
$$ {#eq-hajek-canonical}

When $p_i = p$ for all units, @eq-hajek-canonical can be rewritten as:

$$
\hat{\tau}_{HA} = \frac{\frac{1}{p}\sum_{i=1}^{N}D_iY_i}{\frac{1}{p}\sum_{i=1}^{N}D_i} - \frac{\frac{1}{1-p}\sum_{i=1}^{N}(1-D_i)Y_i}{\frac{1}{1-p}\sum_{i=1}^{N}(1-D_i)}
$$ {#eq-hajek-constant-p}

By cancelling out the $1/p$ and $1/(1-p)$ terms that appear the both the numerator and denominator of the fractions in @eq-hajek-constant-p, we can reduce the Hajek estimator to a form that is equivalent to the DIM estimator:

$$
\hat{\tau}_{HA} = \frac{\sum_{i=1}^{N} D_i Y_i}{\sum_{i=1}^{N} D_i} - \frac{\sum_{i=1}^{N} (1-D_i) Y_i}{\sum_{i=1}^{N} (1-D_i)}
$$ {#eq-hajek-equivalent}

This equivalence should hold for simple random assignment and for complete random assignment because the probability of being assigned to treatment is equal to a fixed constant $p$ for all units under both of these assignment mechanisms. In the case of complete random assignment, $p=N_t / N$.

# Question 3

In this excercise, we analyze data from a study by [Santoro and Broockman (2022)](https://doi.org/10.1126/sciadv.abn5515) examining the effect of conversations between strangers of opposing political parties on measures of polarization. During their conversation, subjects talked about what their perfect day would look like. Subjects assigned to treatment ($D_i=1$) were informed prior to the start of the conversation that their partner was an out-partisan, while those assigned to control ($D_i=0$) were not informed of this. The primary outcome of interest $Y_i$ was self-reported warmth towards outpartisans measured on a feeling thermometer.

## Part A

Here, we compute the ATE using the Neyman difference-in-means estimator (\@eq-dim). The formula for the corresponding Neyman variance estimate is given by:

$$
\hat{\mathbb{V}}\left[\hat{\tau}_{DIM}\right] = \frac{s_t^2}{N_t} + \frac{s_c^2}{N_c}
$$

This estimate of the variance is conservative and will tend to overestimate the uncertainty in estimates of $\hat{\tau}$. This variance estimate can be used to construct confidence intervals as follows:

$$
CI^{1-\alpha}(\hat{\tau}_{DIM}) = \hat{\tau} \pm z_{1-\alpha/2} \sqrt{\hat{\mathbb{V}}\left[\hat{\tau}_{DIM}\right]}
$$

where $\alpha$ is the chosen significance level and $z_{1-\alpha/2}$ is the corresponding quantile of a standard normal distribution.

```{r question-3a, eval=TRUE}

# Load required packages. 
library(tidyverse)

# Set random seed for reproducibility. 
set.seed(42)

# Load data. 
df <- read.csv("data.csv")

# Rename columns to be consistent with notation used elsewhere in this document. 
colnames(df) <- c("Y","D")

# Compute ATE point estimate using Neyman DIM estimator. 
ATE_DIM <- mean(df$Y[df$D==1]) - mean(df$Y[df$D==0])

# Compute standard error using Neyman variance estimator. This estimator is 
# conservative and tends to return values that are larger than the true variance. 
St <- sd(df$Y[df$D==1])
Sc <- sd(df$Y[df$D==0])
Nt <- sum(df$D)
Nc <- sum(1-df$D)
ATE_DIM_VAR <- St^2/Nt + Sc^2/Nc

# Compute 95% confidence interval using Neyman variance estimate. 
alpha <- 0.05
z <- qnorm(1-alpha/2)
ATE_DIM_LB <- ATE_DIM - z*sqrt(ATE_DIM_VAR)
ATE_DIM_UB <- ATE_DIM + z*sqrt(ATE_DIM_VAR)

# Print results
print(paste("ATE point estimate: ",round(ATE_DIM,6)))
print(paste("Neyman variance estimate: ",round(ATE_DIM_VAR,6)))
print(paste("95% confidence interval: ",round(ATE_DIM_LB,6),"-",round(ATE_DIM_UB,6)))


```

## Part B

Here, we compare the confidence intervals computed using the Neyman variance estimator to those constructed via bootstrapping. The bootstrapping procedure can be summarized as follows:

1.  Create a resampled dataset by drawing $N$ samples from the original dataset *with replacement*.

2.  Estimate $\hat{\tau}_{boot}$ by applying the DIM estimator to the resampled data.

3.  Repeat steps (1) and (2) many times. The resulting distribution of $\hat{\tau}_{boot}$ should approximate the sampling distribution of $\hat{\tau}_{DIM}$ and can be used to construct confidence intervals on the average treatment effect.

A nonparametric way to construct bootstrapped confidence intervals is by taking the $\alpha/2$ and $1-\alpha/2$ quantiles of the empirical distribution of $\hat{\tau}_{boot}$.

$$
CI^{1-\alpha} = \left[ Q_{\alpha/2}(\hat{\tau}_{boot}),Q_{1-\alpha/2}(\hat{\tau}_{boot})\right]
$$

Alternatively, because it can be shown that $\hat{\tau}_{boot}$ is normally distributed based on the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) and [sum of normally distributed random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables), we can also construct confidence intervals using the normal approximation:

$$
CI^{1-\alpha} = \hat{\tau}_{DIM} \pm z_{1-\alpha/2} \sqrt{\mathbb{V}\left[\hat{\tau}_{boot}\right]}
$$

```{r question-3b, eval=TRUE}

# Set random seed for reproducibility. 
set.seed(42)

# Specify number of bootstrapping iterations
B <- 1000

# Specify number of samples in dataset
N <- nrow(df)

# Initialize vector to hold bootstrapped estimates of ATE
ATE_BOOT_DIST <- rep(NA,B)

# Repeatedly resample data to get bootstrapped distribution of ATE
for (j in 1:B){
  
  # Resample data with replacement
  df_boot <- slice_sample(df, n = N, replace = TRUE)
  
  # Compute ATE using DIM estimator
  ATE_BOOT_DIST[j] <- mean(df_boot$Y[df_boot$D==1]) - mean(df_boot$Y[df_boot$D==0])
}

# Compute bootstrapped CI using the percentile method
ATE_BOOT_PERC_LB <- quantile(ATE_BOOT_DIST,alpha/2)
ATE_BOOT_PERC_UB <- quantile(ATE_BOOT_DIST,1-alpha/2)

# Compute bootstrapped CI using the normal approximation method
ATE_BOOT_VAR <- var(ATE_BOOT_DIST)
ATE_BOOT_NORM_LB <- ATE_DIM - z*sqrt(ATE_BOOT_VAR)
ATE_BOOT_NORM_UB <- ATE_DIM + z*sqrt(ATE_BOOT_VAR)

# Dump results into a dataframe that we'll use for plotting
results_df <- tibble(
  method=c("Neyman","Bootstrapped - Percentile","Bootstrapped - Normal"),
  estimate=c(ATE_DIM,ATE_DIM,ATE_DIM),
  conf.low=c(ATE_DIM_LB,ATE_BOOT_PERC_LB,ATE_BOOT_NORM_LB),
  conf.high=c(ATE_DIM_UB,ATE_BOOT_PERC_UB,ATE_BOOT_NORM_UB)
)

# Create coefficient plot
p <- ggplot(results_df, aes(x = estimate, y = method)) +
  geom_point(size = 3) +
  geom_errorbar(
    aes(xmin = conf.low, xmax = conf.high),
    height = 0.15
  ) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  labs(
    x = "Average Treatment Effect",
    y = NULL,
    title = "95% Confidence Interval By Method"
  ) +
  theme_minimal()

# Display to console
p

```

As shown in the above plot, our estimate of the ATE is statistically significant at the $\alpha=0.05$ level under all three evaluated methods of constructing confidence intervals.

## Part C

Here, we use Fisher's randomization test (FRT) to construct the distribution of ATE estimates under the sharp null hypothesis:

$$
H_0: Y_i(0) = Y_i(1) = Y_i^{obs} \space \forall i
$$

This is done by repeatedly estimating the ATE under randomly-generated treatment assignment vectors. By comparing our actual ATE estimate to this null distribution, we can compute a p-value which represents the probability of observing a result this extreme under the sharp null hypothesis.

```{r question-3c, eval=TRUE}

# Set random seed for reproducibility. 
set.seed(42)

# Specify probability of treatment assignment
prob_treatment <- mean(df$D)

# Initialize vector to hold estimates of ATE under sharp null hypothesis
ATE_FRT_DIST <- rep(NA,B)

# Compute null distribution of ATE using Fisher randomization approach
for (j in 1:B){
  
  # Randomize treatment assignments
  # (breaks relationship between treatment assignment and outcome of interest)
  D_FRT <- rbinom(N,1,prob_treatment)
  
  # Compute ATE using DIM estimator and randomized treatment assignment vector
  ATE_FRT_DIST[j] <- mean(df$Y[D_FRT==1]) - mean(df$Y[D_FRT==0])
}

# Compute 1-alpha/2 percentile of null ATEs
critical_value = quantile(ATE_FRT_DIST,1-alpha/2)

# Plot null distribution of ATEs against our point estimate
p <- ggplot(data.frame(ATE_FRT_DIST), aes(x = ATE_FRT_DIST)) +
  geom_density(
    fill = "gray70",
    alpha = 0.6,
    linewidth = 0.8
  ) +
  geom_vline(
    xintercept = critical_value,
    linetype = "dashed",
    linewidth = 1
  ) +
  geom_vline(
    xintercept = ATE_DIM,
    linetype = "solid",
    linewidth = 1
  ) +
  labs(
    x = "ATE under sharp null",
    y = "Density",
    title = "Null Distribution of ATE Estimates",
    subtitle = "Dashed line: 97.5th percentile of null; Solid line: observed ATE"
  ) +
  theme_minimal()

# Display to console
p

# Compute p-value for ATE estimate under sharp null
FRT_pval <- mean(abs(ATE_FRT_DIST) > abs(ATE_DIM))
print(paste("P-value of ATE estimate under sharp null hypothesis: ",round(FRT_pval,6)))
```

We can reject the null hypothesis since our observed ATE estimate is more extreme than the $1-\alpha/2$ quantile of ATE estimates under the sharp null hypothesis. This is consistent with our findings from Part B, in which the weak null hypothesis of no average treatment effect was rejected. If the sharp null hypothesis were true, then we would also expect the weak null hypothesis to be satisfied.

---
title: "POLI 784 - Problem Set #1"
author: "Emily Arnsberg & Kieran Fitzmason"
date: today
format: html
---

# Question 3

## Background

In this problem, we simulate a classical two-arm study design to estimate the causal effect of access to an AI coding assistant ($Z_i$) on the weekly number of features shipped per software developer ($Y_i$). The data is assumed to be generated by the following process: 

$$ Y_i = \tau*Z_i + X_i + U_i $$

where $\tau$ represents the causal effect we are interested in estimating (the estimand); $X_i$ is a covariate drawn from a discrete uniform distribution that can take on a value of 0, 1, or 2; and $U_i$ is a variable drawn from a standard uniform distribution that represents unobserved heterogeneity between units. 

## Part A

Here, we use the `DeclaredDesign` package to simulate a classical two-arm experiment. A total of $n=150$ subjects are drawn from the population of $N=3000$ software developers and assigned to treatment or control via a bernoulli trial with $p=0.5$. A difference-in-means estimator will be used to calculate $\hat{\tau}$; the "true" value of this parameter used within the data generating process is set to $\tau = 0.25$. 

```{r question-3a, eval=TRUE}

# Load required packages 
library(DeclareDesign)
library(tidyverse)

# Set random seed for reproducibility
set.seed(42)

# Specify assumed value of average treatment effect 
tau = 0.25

# Specify model of data-generating process

model <- declare_model(N=3000,
                       U=runif(N),
                       X=sample(c(0,1,2),N,replace=TRUE),
                       potential_outcomes(Y ~ tau*Z + X + U))

# Specify inquiry
inquiry <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))

# Specify sampling strategy, treatment assignment, and measurement procedure
# Together these comprise your "data strategy"

n <- 150 # Sample size 

sampling_procedure <- declare_sampling(S = complete_rs(N=N,n=n))
randomization_procedure <- declare_assignment(Z = simple_ra(N=N, prob=0.5))
measurement_procedure <- declare_measurement(Y = reveal_outcomes(Y ~ Z))
data_strategy <- sampling_procedure + randomization_procedure + measurement_procedure

# Specify answer strategy / method for generating parameter estimates from the data
answer_strategy <- declare_estimator(Y ~ Z, .method=difference_in_means, inquiry="ATE")

# Combine different components of research design into a "declaration" object
declaration_3A <- model + inquiry + data_strategy + answer_strategy

# Display simulated data
head(draw_data(design = declaration_3A))

```

## Part B

Next, we use the `diagnose_design` function to evaluate the expected performance of our study design. This is done by simulating the outcomes of our experiment 1000 times and comparing the sampling distribution of $\hat{\tau}$ against the "true" value $\tau=0.25$. This sampling distribution can then be used to calculate performance metrics such as bias, power, and coverage. Bias is defined as the the expected difference between our estimate and estimand ($\mathbb{E}[\hat{\tau} - \tau]$). Power is defined as the probability of correctly rejecting the null hypothesis ($H_0: \tau=0$) given our study design and desired significance level ($\alpha=0.05$). Coverage is defined as the proportion of $100(1-\alpha)\%$ confidence intervals that contain the true value of our estimand $\tau=0.25$. 


```{r question-3b, eval=TRUE}

# Specify significance level to use in hypothesis testing. 
alpha <- 0.05

# Specify metrics used to evaluate study design. Note that we could have just 
# used the default metrics, which include bias, power, and coverage; however, 
# specifying them manually makes their definitions explicit. For more examples,
# please see the examples associated with the documentation for declare_diagnosands. 

diagnosands <- declare_diagnosands(
    mean_estimand = mean(estimand),
    mean_estimate = mean(estimate),
    bias = mean(estimate - estimand),
    power = mean(p.value <= alpha),
    coverage = mean(estimand <= conf.high & estimand >= conf.low)
  )

# Calculate value of metrics under 1000 realizations of experiment
diagnosis <- diagnose_design(declaration_3A, diagnosands=diagnosands,sims=1000)

# Display results to console
diagnosis
```

Based on the results of our simulated experiments, we can observe that our estimator $\hat{\tau}$ is unbiased. In addition, the observed coverage probability of our confidence intervals (95%) is exactly what we would expect for our chosen significance level $\alpha=0.05$. However, we also find that our current study design is woefully underpowered: the null hypothesis (which we know to be false) was only rejected in 40% of simulated experiments. This issue would likely be serious enough to warrant a redesign of our study. 

## Part C

Typically, the simplest way for researchers to increase a study's power is by increasing the sample size. Here, we use the `redesign` function in the `DeclaredDesign` package to explore how the power of the study described in parts A and B varies as a function of sample size. 


```{r question-3c, eval=TRUE}

# Explore alternative study designs with sample sizes between 150 and 400
sample_size_options <- seq(from=150,to=400,by=50)
designs_by_n <- redesign(declaration_3A, n = sample_size_options)

# Evaluate performance of alternative designs using same metrics as before. 
# To reduce computation time, cut number of simulations per design in half. 
diagnose_by_n <- diagnose_designs(designs_by_n, diagnosands=diagnosands,sims=500)

# Display results to console
diagnose_by_n

# Create plot
# (!) come back to this


```






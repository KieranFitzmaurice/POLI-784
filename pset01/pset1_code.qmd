---
title: "POLI 784 - Problem Set #1"
author: "Emily Arnsberg & Kieran Fitzmason"
date: today
format: html
---

# Question 3

## Background

In this problem, we simulate a classical two-arm study design to estimate the causal effect of access to an AI coding assistant ($Z_i$) on the weekly number of features shipped per software developer ($Y_i$). The data is assumed to be generated by the following process:

$$ Y_i = \tau*Z_i + X_i + U_i $$

where $\tau$ represents the causal effect we are interested in estimating (the estimand); $X_i$ is a covariate drawn from a discrete uniform distribution that can take on a value of 0, 1, or 2; and $U_i$ is a variable drawn from a standard uniform distribution that represents unobserved heterogeneity between units.

## Part A

Here, we use the `DeclaredDesign` package to simulate a classical two-arm experiment. A total of $n=150$ subjects are drawn from the population of $N=3000$ software developers and assigned to treatment or control via a bernoulli trial with $p=0.5$. A difference-in-means estimator will be used to calculate $\hat{\tau}$; the "true" value of this parameter used within the data generating process is set to $\tau = 0.25$.

```{r question-3a, eval=TRUE}

# Load required packages 
library(DeclareDesign)
library(tidyverse)
library(knitr)
library(latex2exp)

# Set random seed for reproducibility
set.seed(42)

# Specify fixed parameters

N <- 3000       # Population size
n <- 150        # Sample size
tau <- 0.25   # Assumed value of average treatment effect 

# Specify model of data-generating process

model <- declare_model(N=N,
                       U=runif(N),
                       X=sample(c(0,1,2),N,replace=TRUE),
                       potential_outcomes(Y ~ tau*Z + X + U))

# Specify inquiry
inquiry <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))

# Specify sampling strategy, treatment assignment, and measurement procedure
# Together these comprise your "data strategy"
sampling_procedure <- declare_sampling(S = complete_rs(N=N,n=n))
randomization_procedure <- declare_assignment(Z = simple_ra(N=N, prob=0.5))
measurement_procedure <- declare_measurement(Y = reveal_outcomes(Y ~ Z))
data_strategy <- sampling_procedure + randomization_procedure + measurement_procedure

# Specify answer strategy / method for generating parameter estimates from the data
answer_strategy <- declare_estimator(Y ~ Z, .method=difference_in_means, inquiry="ATE")

# Combine different components of research design into a "declaration" object
declaration_3A <- model + inquiry + data_strategy + answer_strategy

# Display simulated data
head(draw_data(design = declaration_3A))

```

## Part B

Next, we use the `diagnose_design` function to evaluate the expected performance of our study design. This is done by simulating the outcomes of our experiment 1000 times and comparing the sampling distribution of $\hat{\tau}$ against the "true" value $\tau=0.25$. This sampling distribution can then be used to calculate performance metrics such as bias, power, and coverage. Bias is defined as the the expected difference between our estimate and estimand ($\mathbb{E}[\hat{\tau} - \tau]$). Power is defined as the probability of correctly rejecting the null hypothesis ($H_0: \tau=0$) given our study design and desired significance level ($\alpha=0.05$). Coverage is defined as the proportion of $100(1-\alpha)\%$ confidence intervals that contain the true value of our estimand $\tau=0.25$.

```{r question-3b, eval=TRUE}

# Specify significance level to use in hypothesis testing. 
alpha <- 0.05

# Specify metrics used to evaluate study design. Note that we could have just 
# used the default metrics, which include bias, power, and coverage; however, 
# specifying them manually makes their definitions explicit. For more examples,
# please see the examples associated with the documentation for declare_diagnosands. 

diagnosands <- declare_diagnosands(
    mean_estimand = mean(estimand),
    mean_estimate = mean(estimate),
    bias = mean(estimate - estimand),
    power = mean(p.value <= alpha),
    coverage = mean(estimand <= conf.high & estimand >= conf.low)
  )

# Calculate value of metrics under 1000 realizations of experiment
diagnosis <- diagnose_design(declaration_3A, diagnosands=diagnosands,sims=1000)

# Tidy to summary dataframe
diagnosis <- tidy(diagnosis) %>% filter(diagnosand %in% c('bias','power','coverage'))

# Save as latex table
table_data <- pivot_wider(data=diagnosis, 
                          id_cols = c(design), 
                          values_from = estimate, 
                          names_from = diagnosand)

latex_table <- kable(table_data, format="latex", booktabs=TRUE)
writeLines(latex_table, con = "table_3B.tex")

# Display results to console
table_data

```

Based on the results of our simulated experiments, we can observe that our estimator $\hat{\tau}$ is unbiased. In addition, the observed coverage probability of our confidence intervals (95%) is exactly what we would expect for our chosen significance level $\alpha=0.05$. However, we also find that our current study design is woefully underpowered: the null hypothesis (which we know to be false) was only rejected in 40% of simulated experiments. This issue would likely be serious enough to warrant a redesign of our study.

## Part C

Typically, the simplest way for researchers to increase a study's power is by increasing the sample size. Here, we use the `redesign` function in the `DeclaredDesign` package to explore how the power, bias, and coverage of the study described in parts A and B varies as a function of sample size.

```{r question-3c, eval=TRUE}

# Explore alternative study designs with sample sizes between 150 and 400
sample_size_options <- seq(from=150,to=400,by=50)
designs_by_n <- redesign(declaration_3A, n = sample_size_options)

# Evaluate performance of alternative designs using same metrics as before. 
# To reduce computation time, cut number of simulations per design in half. 
diagnose_by_n <- diagnose_designs(designs_by_n,
                                  diagnosands=diagnosands,
                                  sims=500)

# Tidy result into a summary dataframe and filter by quantities of interest
diagnose_by_n <- tidy(diagnose_by_n) %>% filter(diagnosand %in% c('bias','power','coverage'))

# Save as latex table
table_data <- pivot_wider(data=diagnose_by_n, 
                          id_cols = c(design,n), 
                          values_from = estimate, 
                          names_from = diagnosand)

latex_table <- kable(table_data, format="latex", booktabs=TRUE)
writeLines(latex_table, con = "table_3C.tex")

# Create figure showing how bias, power, and coverage vary 
# as a function of sample size. 
p <- ggplot(diagnose_by_n,
       aes(x = n,
           y = estimate,
           ymin = conf.low,
           ymax = conf.high,
           color = diagnosand)
       ) + 
  geom_line() +
  geom_point() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = diagnosand),
              alpha = 0.15,
              color = NA) +
  labs(
    x = "Sample size (n)",
    y = "Estimate",
    title = "Bias, power, and coverage vs. sample size"
  ) +
  theme_minimal()

# Save figure as png
ggsave(
  filename = "figure_3C.png",
  plot = p,
  width = 6,
  height = 4,
  dpi = 350
)

# Display results to console
table_data
p

```

As shown in the plot above, the power of the study is sensitive to the sample size while the bias and coverage is unaffected. This is a reassuring finding which tells us that our estimator remains unbiased and that the assumptions used to construct confidence intervals hold across the range of sample sizes we are considering. It also tells us that we should aim for a sample size of $n = 400$ in order to meet the commonly-used benchmark of 80% power given our effect size of interest.

## Part D

Treating $\tau$ as a fixed value in our simulation implies a high level of prior knowledge, which may or may not be realistic. Here, we explore the impact of uncertainty regarding the value of our estimand on the statistical power of our study design by conducting a new simulation where $\tau$ is modeled as a uniform random variable that lies between 0 and 0.5:

$$\tau \sim U(0,0.5)$$

```{r question-3d, eval=TRUE}

# Evaluate how treating tau as a random variable instead affects our study design. 

# Specify new model of data-generating process

model_3D <- declare_model(N=N,
                          U=runif(N),
                          X=sample(c(0,1,2),N,replace=TRUE),
                          tau=runif(1,0,0.5),
                          potential_outcomes(Y ~ tau*Z + X + U))

# Keep the other aspects of our study the same as in 3A
declaration_3D <- model_3D + inquiry + data_strategy + answer_strategy

# Now simulate data twice to show how tau can take on different values in 
# different "states of the world". 
head(draw_data(declaration_3D))
head(draw_data(declaration_3D))

# Diagnose design to see how value of tau affects power
diagnosis_3D <- diagnose_design(declaration_3D, diagnosands=diagnosands,sims=1000)

# Get simulations
simulations_3D <- get_simulations(diagnosis_3D)

# Create a new binary column denoting whether we correctly 
# rejected the null hypothesis of tau = 0. 
simulations_3D$reject_null <- as.numeric(simulations_3D$p.value < alpha)

# Create plot of statistical power vs. simulated value of tau. 
# Use LOESS regression to smooth out noise in plot so that we can 
# visualize power as a smooth function of tau. 

p <- ggplot(simulations_3D) + 
  
  # LOESS regression 
  stat_smooth(aes(x=estimand, y=reject_null), 
              method = 'loess', 
              color = "#3564ED", 
              fill = "#72B4F3", 
              formula = 'y ~ x') +
  
  # Plot convention power threshold of 80% 
  geom_hline(yintercept = 0.8, color = "#C6227F", linetype = "dashed") +
  annotate("text", x = 0, y = 0.85, label = "Conventional power threshold = 0.8", hjust = 0, color = "#C6227F") + 
  
  # Format axes and labels
  scale_y_continuous(breaks = seq(0, 1, 0.2)) +
  coord_cartesian(ylim = c(0, 1)) +
  theme(legend.position = "none") +
  labs(x = TeX("Estimand: average treatment effect $\\tau$"),
       y = "Diagnosand: statistical power",
       title = "Power vs. effect size ") +
  theme_minimal()

# Save plot as png
ggsave(
  filename = "figure_3D.png",
  plot = p,
  width = 6,
  height = 4,
  dpi = 350
)

# Display results to console
p
```

As shown in the above plot, the power of our study design is highly sensitive to the size of the average treatment effect $\tau$. With a sample size of $n=150$, the minimum effect size that our study can detect with at least 80% power is approximately $\tau=0.4$.

## Part E


```{r question-3e, eval=TRUE}

# Vary sample size and effect size simultaneously
designs_3E <- redesign(declaration_3A, n = c(150,250,350), tau = c(0.2,0.3,0.4))

# Evaluate performance under different scenarios 
diagnoses_3E <- diagnose_designs(designs_3E,
                                 diagnosands=diagnosands,
                                 sims=500)

# Tidy result into a summary dataframe and filter by quantities of interest
diagnoses_3E <- tidy(diagnoses_3E) %>% filter(diagnosand %in% c('bias','power','coverage'))

# Save as latex table
table_data <- pivot_wider(data=diagnoses_3E, 
                          id_cols = c(design,n,tau), 
                          values_from = estimate, 
                          names_from = diagnosand)

latex_table <- kable(table_data, format="latex", booktabs=TRUE)
writeLines(latex_table, con = "table_3E.tex")

# Create figure showing power varies as a function of sample size 
# stratified by effect size. 
table_data$tau <- as.factor(table_data$tau)

p <- ggplot(table_data,
            aes(x = n,
                y = power,
                color = tau)) + 
  geom_line() +
  geom_point() +
  scale_y_continuous(breaks = seq(0, 1, 0.2)) +
  coord_cartesian(ylim = c(0, 1),xlim = c(150,350)) +
  labs(
    x = "Sample size (n)",
    y = "Power",
    title = "Power vs. sample size by effect size") +
  theme_minimal()

# Save figure as png
ggsave(
  filename = "figure_3E.png",
  plot = p,
  width = 6,
  height = 4,
  dpi = 350
)

# Displate results to console
table_data
p
```

The "optimal" sample size for the company's study is likely to depend on the following factors: 

1. The value of the average treatment effect $\tau$. 
2. The cost of running the study for a given sample size $n$. 
3. The cost of making a type I error (i.e., adopting an AI tool that does not provide meaningful productivity gains.)
4. The cost of making a type II error (i.e., failing to adopt an AI tool that provides meaningful productivity gains.)

If we were able to quantify items 2-4, we could create a custom `diagnosand` that represents the expected net monetary benefits to the company of making a decision based on a given study design. This could then be used to determine the sample size that maximizes expected profits. 

In our opinion, the expected costs (in terms of foregone profits) associated with making a type I or type II error are likely to far exceed the cost of running most of the study designs considered in this analysis, particularly if the company is considering a long-term contract with the provider of the AI tool. As such, we would recommend that the company conduct their study using a larger sample size of $n=350$ or more. This sample size should be sufficient to detect an effect size of $\tau = 0.25$ with almost 80% power (Fig. 3c) and an effect size of $\tau > 0.3$ with over 90% power (Fig. 3d). 

